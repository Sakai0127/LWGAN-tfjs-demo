{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "lightweightGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UM41Qtcmd-l6",
        "BDflUNDqHzq-",
        "elYLiKcV68aa",
        "jfsCoi1G6ypn"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhA6gewxh9Zr"
      },
      "source": [
        "# [lightweught GAN(by lucidrains)](https://github.com/lucidrains/lightweight-gan)\r\n",
        "tensorflow implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM41Qtcmd-l6"
      },
      "source": [
        "# Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzeGkSVS4_FH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e658df85-bbf6-4b5a-d803-5a8193dce8af"
      },
      "source": [
        "!pip install tfa-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfa-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/30b7476e9b80b82f7e541077809e232154b629b65108f949fdced11f0d8e/tfa_nightly-0.13.0.dev20210213171101-cp36-cp36m-manylinux2010_x86_64.whl (706kB)\n",
            "\r\u001b[K     |▌                               | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 10.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 163kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 174kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 215kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 225kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 235kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 266kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 276kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 296kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 327kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 337kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 348kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 378kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 389kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 399kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 409kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 430kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 450kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 460kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 471kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 481kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 491kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 501kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 512kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 522kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 532kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 542kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 552kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 563kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 583kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 593kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 604kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 614kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 624kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 634kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 645kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 655kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 665kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 675kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 686kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 696kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 716kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tfa-nightly) (2.7.1)\n",
            "Installing collected packages: tfa-nightly\n",
            "Successfully installed tfa-nightly-0.13.0.dev20210213171101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MSs7twceFt_"
      },
      "source": [
        "import os\r\n",
        "import pickle\r\n",
        "from random import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_addons as tfa\r\n",
        "\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVT1P7vfYKn0"
      },
      "source": [
        "## Custom Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx3voICAYcxz"
      },
      "source": [
        "class GSA(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, output_filters, n_keys=64, heads=8, **kwargs):\r\n",
        "        super(GSA, self).__init__(**kwargs)\r\n",
        "\r\n",
        "        self.output_filters = output_filters\r\n",
        "        self.n_keys = n_keys\r\n",
        "        self.heads = heads\r\n",
        "\r\n",
        "        hidden = n_keys * heads\r\n",
        "        self.conv1 = tf.keras.layers.Conv2D(hidden*3, 1, kernel_initializer=kernel_initializer, use_bias=False)\r\n",
        "        self.out_conv = tf.keras.layers.Conv2D(output_filters, 1, kernel_initializer=kernel_initializer)\r\n",
        "    \r\n",
        "    def call(self, inputs, **kwargs):\r\n",
        "        input_shape = inputs.shape\r\n",
        "        q, k, v = tf.split(self.conv1(inputs), 3, axis=3)\r\n",
        "        s = [-1, input_shape[1]*input_shape[2], self.heads, self.n_keys]\r\n",
        "        q, k, v = tf.reshape(q, s), tf.reshape(k, s), tf.reshape(v, s)\r\n",
        "        k = tf.nn.softmax(k, axis=1)\r\n",
        "        q = tf.nn.softmax(q, axis=3)\r\n",
        "        context = tf.einsum('bihj, bihk -> bjhk', k, v)\r\n",
        "        out = tf.einsum('bihc, bjhi -> bjhc', context, q)\r\n",
        "        out = tf.reshape(out, [-1, input_shape[1], input_shape[2], self.heads*self.n_keys])\r\n",
        "        out = self.out_conv(out)\r\n",
        "        return out\r\n",
        "    \r\n",
        "    def get_config(self):\r\n",
        "        base_config = super(GSA, self).get_config()\r\n",
        "        config = dict(output_filters=self.output_filters, n_keys=self.n_keys, heads=self.heads)\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "class ScalingLayer(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super(ScalingLayer, self).__init__(*args, **kwargs)\r\n",
        "    \r\n",
        "    def build(self, *args, **kwargs):\r\n",
        "        self.weight = self.add_weight(name='scale',\r\n",
        "                                      shape=(1,),\r\n",
        "                                      dtype=tf.float32,\r\n",
        "                                      initializer=tf.keras.initializers.Constant(1e-3),\r\n",
        "                                      trainable=True,\r\n",
        "                                      aggregation=tf.compat.v1.VariableAggregation.MEAN)\r\n",
        "        super(ScalingLayer, self).build(*args, **kwargs)\r\n",
        "    \r\n",
        "    def call(self, inputs, **kwargs):\r\n",
        "        return inputs * self.weight\r\n",
        "\r\n",
        "class UpFIR2d(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, scale=2, k=None, gain=1.0, up=False, down=False, conv=False, conv_k_size=None, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\r\n",
        "        super(UpFIR2d, self).__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)\r\n",
        "        \r\n",
        "        self.scale = scale\r\n",
        "        self.gain = gain\r\n",
        "        self.up = up\r\n",
        "        self.down = down\r\n",
        "        assert not(self.up == self.down)\r\n",
        "        self.conv = conv\r\n",
        "        self.conv_k_size = conv_k_size\r\n",
        "        if k is None:\r\n",
        "            self.k = (1,) * self.scale\r\n",
        "        else:\r\n",
        "            self.k = tuple(k)\r\n",
        "        f = self._get_filter()\r\n",
        "        self.filter = self.add_weight(name='resample_kernel',\r\n",
        "                                      shape=f.shape,\r\n",
        "                                      dtype=tf.float32,\r\n",
        "                                      initializer=tf.keras.initializers.Constant(f),\r\n",
        "                                      trainable=False,\r\n",
        "                                      aggregation=tf.VariableAggregation.MEAN)\r\n",
        "    \r\n",
        "    def _get_filter(self):\r\n",
        "        k = np.asarray(self.k, dtype=np.float32)\r\n",
        "        k = np.outer(k, k)\r\n",
        "        k /= np.sum(k)\r\n",
        "        if self.up:\r\n",
        "            k = k * (self.gain * (self.scale ** 2))\r\n",
        "        elif self.down:\r\n",
        "            k = k * self.gain\r\n",
        "        return k[:,:,np.newaxis, np.newaxis]\r\n",
        "    \r\n",
        "    def _get_params(self):\r\n",
        "        p = self.filter.shape[0] - self.scale\r\n",
        "        if self.conv:\r\n",
        "            if self.up:\r\n",
        "                p -= (self.conv_k_size - 1)\r\n",
        "            elif self.down:\r\n",
        "                p += (self.conv_k_size - 1)\r\n",
        "        if self.up:\r\n",
        "            up = 1 if self.conv else self.scale\r\n",
        "            down = 1\r\n",
        "            p0 = (p+1)//2+self.scale-1\r\n",
        "            p1 = p//2 + 1 if self.conv else p//2\r\n",
        "        elif self.down:\r\n",
        "            up = 1\r\n",
        "            down = 1 if self.conv else self.scale\r\n",
        "            p0 = (p+1)//2\r\n",
        "            p1 = p//2\r\n",
        "        return dict(up=up, down=down, p0=p0, p1=p1)\r\n",
        "    \r\n",
        "    def _upfirdn2d_op(self, x, upx, upy, downx, downy, px0, px1, py0, py1):\r\n",
        "        xs = tf.shape(x)\r\n",
        "        #x = tf.reshape(x, [-1, x.shape[1], 1, x.shape[2], 1, x.shape[3]])\r\n",
        "        x = tf.reshape(x, [-1, xs[1], 1, xs[2], 1, xs[3]])\r\n",
        "        x = tf.pad(x, [[0, 0], [0, 0], [0, upy-1], [0, 0], [0, upx-1], [0, 0]])\r\n",
        "        x = tf.reshape(x, [-1, xs[1]*upy, xs[2]*upx, xs[3]])\r\n",
        "\r\n",
        "        x = tf.pad(x, [[0, 0], [max(py0, 0), max(py1, 0)], [max(px0, 0), max(px1, 0)], [0, 0]])\r\n",
        "        x = x[:, max(-py0, 0):tf.shape(x)[1] - max(-py1, 0), max(-px0, 0):tf.shape(x)[2] - max(-px1, 0), :]\r\n",
        "\r\n",
        "        x = tf.nn.depthwise_conv2d(x, tf.tile(self.filter, [1, 1, tf.shape(x)[-1], 1]), strides=[1, 1, 1, 1], padding='VALID')\r\n",
        "        return x[:, ::downy, ::downx, :]\r\n",
        "\r\n",
        "    def _upfirdn2d(self, x, up=1, down=1, p0=0, p1=0):\r\n",
        "        x = self._upfirdn2d_op(x, upx=up, upy=up, downx=down, downy=down, px0=p0, px1=p1, py0=p0, py1=p1)\r\n",
        "        return x\r\n",
        "    \r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        if self.up:\r\n",
        "            return (None, input_shape[1]*self.scale, input_shape[2]*self.scale, input_shape[3])\r\n",
        "        elif self.down:\r\n",
        "            return (None, input_shape[1]//self.scale, input_shape[2]//self.scale, input_shape[3])\r\n",
        "    \r\n",
        "    def call(self, inputs, **kwargs):\r\n",
        "        params = self._get_params()\r\n",
        "        x = self._upfirdn2d(inputs, **params)\r\n",
        "        x.set_shape(self.compute_output_shape(inputs.shape))\r\n",
        "        return x\r\n",
        "    \r\n",
        "    def get_config(self):\r\n",
        "        basse_config = super(UpFIR2d, self).get_config()\r\n",
        "        config = dict(\r\n",
        "            scale=self.scale,\r\n",
        "            gain=self.gain,\r\n",
        "            up=self.up,\r\n",
        "            down=self.down,\r\n",
        "            conv=self.conv,\r\n",
        "            conv_k_size=self.conv_k_size,\r\n",
        "            k=self.k,\r\n",
        "        )\r\n",
        "        return dict(list(basse_config.items()) + list(config.items()))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdOUpJaHYTdr"
      },
      "source": [
        "##Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5p6SDJqhhnh"
      },
      "source": [
        "# kernel_initializer = tf.keras.initializers.random_normal(mean=0.0, stddev=0.02)\r\n",
        "# kernel_initializer = tf.keras.initializers.glorot_uniform()\r\n",
        "kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2, mode='fan_in', distribution='truncated_normal')\r\n",
        "\r\n",
        "def global_context(input, output_channels):\r\n",
        "    #Squeeze and Excitation with simple attention\r\n",
        "    input_shape = input.shape\r\n",
        "    key = tf.keras.layers.Conv2D(1, 1)(input)\r\n",
        "    key = tf.reshape(key, [-1, input_shape[1]*input_shape[2], 1])\r\n",
        "    key = tf.nn.softmax(key, axis=1)\r\n",
        "    flat_input = tf.reshape(input, [-1, input_shape[1]*input_shape[2], input_shape[3]])\r\n",
        "    attn = tf.matmul(key, flat_input, transpose_a=True)\r\n",
        "    attn = tf.expand_dims(attn, axis=1)\r\n",
        "    out = tf.keras.layers.Conv2D(max(3, output_channels//2), 1, kernel_initializer=kernel_initializer)(attn)\r\n",
        "    out = tf.nn.leaky_relu(out, alpha=0.1)\r\n",
        "    out = tf.keras.layers.Conv2D(output_channels, 1, kernel_initializer=kernel_initializer)(out)\r\n",
        "    out = tf.nn.sigmoid(out)\r\n",
        "    return out\r\n",
        "\r\n",
        "def global_self_attention(x, output_filters, n_keys=64, heads=8):\r\n",
        "    return GSA(output_filters, n_keys, heads)(x)\r\n",
        "\r\n",
        "def blur2D(input):\r\n",
        "    blur_filter = np.array([1., 2., 1.])\r\n",
        "    blur_filter = blur_filter[:, np.newaxis, np.newaxis, np.newaxis] * blur_filter[np.newaxis, :, np.newaxis, np.newaxis]\r\n",
        "    blur_filter = blur_filter / np.sum(np.abs(blur_filter))\r\n",
        "    blur_filter = np.tile(blur_filter, [1, 1, input.shape[-1], 1])\r\n",
        "    pad = tf.pad(input, [[0, 0], [1, 1], [1, 1], [0, 0]])\r\n",
        "    conv = tf.keras.layers.DepthwiseConv2D(3, padding='valid', use_bias=False, depthwise_initializer=tf.keras.initializers.Constant(blur_filter), trainable=False)\r\n",
        "    # out = tf.nn.depthwise_conv2d(pad, tf.constant(blur_filter, dtype=tf.float32), (1, 1, 1, 1), 'VALID')\r\n",
        "    return conv(pad)\r\n",
        "\r\n",
        "def batchnorm(x):\r\n",
        "    return tf.keras.layers.experimental.SyncBatchNormalization(momentum=0.9, epsilon=1e-4)(x)\r\n",
        "    # return tf.keras.layers.BatchNormalization()(x)\r\n",
        "\r\n",
        "def glu(x):\r\n",
        "    x1, x2 = tf.split(x, 2, axis=3)\r\n",
        "    return x1 * tf.nn.sigmoid(x2)\r\n",
        "\r\n",
        "def upscale(input, filters, attn_res=[]):\r\n",
        "    if input.shape[1] in attn_res:\r\n",
        "        input = ScalingLayer()(GSA(input.shape[-1])(input)) + input\r\n",
        "    x = tf.keras.layers.UpSampling2D()(input)\r\n",
        "    x = blur2D(x)\r\n",
        "    x = tf.keras.layers.Conv2D(filters*2, 3, padding='SAME', kernel_initializer=kernel_initializer)(x)\r\n",
        "    x = batchnorm(x)\r\n",
        "    x = glu(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "def downscale(input, filters, attn_res=[]):\r\n",
        "    if input.shape[1] in attn_res:\r\n",
        "        input = ScalingLayer()(GSA(input.shape[-1])(input)) + input\r\n",
        "    x = blur2D(input)\r\n",
        "\r\n",
        "    x1 = tf.keras.layers.Conv2D(filters, 4, 2, padding='SAME', kernel_initializer=kernel_initializer)(x)\r\n",
        "    x1 = tf.nn.leaky_relu(x1, 0.1)\r\n",
        "    x1 = tf.keras.layers.Conv2D(filters, 3, padding='SAME', kernel_initializer=kernel_initializer)(x1)\r\n",
        "    x1 = tf.nn.leaky_relu(x1, 0.1)\r\n",
        "\r\n",
        "    x2 = tf.keras.layers.AveragePooling2D()(x)\r\n",
        "    x2 = tf.keras.layers.Conv2D(filters, 1, kernel_initializer=kernel_initializer)(x2)\r\n",
        "    x2 = tf.nn.leaky_relu(x2, 0.1)\r\n",
        "    return x1 + x2\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ikAt259U3m"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDflUNDqHzq-"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXbZXKZC9c-y"
      },
      "source": [
        "def create_generator(img_size, latent_dims=256, filter_max=512, attn_res=[]):\r\n",
        "    # img_size must be in [256, 512, 1024]\r\n",
        "    inp = tf.keras.Input(shape=(latent_dims,), dtype=tf.float32)\r\n",
        "    x = tf.reshape(inp, [-1, 1, 1, latent_dims])\r\n",
        "    with tf.keras.backend.name_scope('Initial_conv'):\r\n",
        "        x = tf.keras.layers.Conv2DTranspose(latent_dims*2, 4, kernel_initializer=kernel_initializer)(x)\r\n",
        "        x = batchnorm(x)\r\n",
        "        x = glu(x)\r\n",
        "    x = tf.nn.l2_normalize(x, axis=3)\r\n",
        "    filter_num = {\r\n",
        "        8    : 512,\r\n",
        "        16   : 512,\r\n",
        "        32   : 256,\r\n",
        "        64   : 128,\r\n",
        "        128  :  64,\r\n",
        "        256  :  32,\r\n",
        "        512  :  16,\r\n",
        "        1024 :   8\r\n",
        "    }\r\n",
        "    sle_resolution_pairs = {16 : 128, 32 : 256, 64 : 512, 128 : 1024}\r\n",
        "    sle_feature = []\r\n",
        "    with tf.keras.backend.name_scope('Upsample_to_8x8'):\r\n",
        "        x = upscale(x, filters=min(filter_num[8], filter_max), attn_res=attn_res)\r\n",
        "    with tf.keras.backend.name_scope('Upsample_to_16x16'):\r\n",
        "        x = upscale(x, filters=min(filter_num[16], filter_max), attn_res=attn_res)\r\n",
        "        sle_feature.append(x)\r\n",
        "    with tf.keras.backend.name_scope('Upsample_to_32x32'):\r\n",
        "        x = upscale(x, filters=min(filter_num[32], filter_max), attn_res=attn_res)\r\n",
        "        sle_feature.append(x)\r\n",
        "    with tf.keras.backend.name_scope('Upsample_to_64x64'):\r\n",
        "        x = upscale(x, filters=min(filter_num[64], filter_max), attn_res=attn_res)\r\n",
        "        if img_size > 256:\r\n",
        "            sle_feature.append(x)\r\n",
        "    with tf.keras.backend.name_scope('Upsample_to_128x128'):\r\n",
        "        x = upscale(x, filters=min(filter_num[128], filter_max), attn_res=attn_res)\r\n",
        "        if img_size > 512:\r\n",
        "            sle_feature.append(x)\r\n",
        "    lr_model = tf.keras.Model(inp, [x]+sle_feature, name='Low_res_part')\r\n",
        "\r\n",
        "    feature128 = tf.keras.Input(shape=x.shape[1:])\r\n",
        "    sle_feature_input = [tf.keras.Input(shape=f.shape[1:]) for f in sle_feature]\r\n",
        "    x = feature128 * global_context(sle_feature_input[0], filter_num[sle_resolution_pairs[16]])\r\n",
        "    with tf.keras.backend.name_scope('Upsample_to_256x256'):\r\n",
        "        x = upscale(x, filters=min(filter_num[256], filter_max), attn_res=attn_res)\r\n",
        "        x = x * global_context(sle_feature_input[1], filter_num[sle_resolution_pairs[32]])\r\n",
        "    if img_size > 256:\r\n",
        "        with tf.keras.backend.name_scope('Upsample_to_512x512'):\r\n",
        "            x = upscale(x, filters=min(filter_num[512], filter_max), attn_res=attn_res)\r\n",
        "            x = x * global_context(sle_feature_input[0], filter_num[sle_resolution_pairs[64]])\r\n",
        "    if img_size > 512:\r\n",
        "        with tf.keras.backend.name_scope('Upsample_to_1024x1024'):\r\n",
        "            x = upscale(x, filters=min(filter_num[1024], filter_max), attn_res=attn_res)\r\n",
        "            x = x * global_context(sle_feature_input[0], filter_num[sle_resolution_pairs[128]])\r\n",
        "    x = tf.keras.layers.Conv2D(3, 3, padding='SAME', kernel_initializer=kernel_initializer)(x)\r\n",
        "    # x = tf.nn.tanh(x)\r\n",
        "    hr_model = tf.keras.Model([feature128]+sle_feature_input, x, name='High_res_part')\r\n",
        "    input = tf.keras.Input(shape=(latent_dims,), dtype=tf.float32)\r\n",
        "    return tf.keras.Model(input, hr_model(lr_model(input)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elYLiKcV68aa"
      },
      "source": [
        "## simple decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hRvpRNZ7A9G"
      },
      "source": [
        "def simple_decoder(x, n_upsample=4):\r\n",
        "    for i in range(n_upsample):\r\n",
        "        filters = 3 if i == (n_upsample-1) else x.shape[-1]//2\r\n",
        "        x = tf.keras.layers.UpSampling2D()(x)\r\n",
        "        x = tf.keras.layers.Conv2D(filters*2, 3, padding='same', kernel_initializer=kernel_initializer)(x)\r\n",
        "        x = glu(x)\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfsCoi1G6ypn"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG9KkqaE61P7"
      },
      "source": [
        "def create_discriminator(img_size, filter_max=512, attn_res=[]):\r\n",
        "    # img_size must be in [256, 512, 1024]\r\n",
        "    filter_num = {8    : 512,\r\n",
        "                  16   : 256,\r\n",
        "                  32   : 128,\r\n",
        "                  64   :  64,\r\n",
        "                  128  :  32,\r\n",
        "                  256  :  16,\r\n",
        "                  512  :   8,\r\n",
        "                  1024 :   8}\r\n",
        "    \r\n",
        "    inp = tf.keras.Input(shape=(img_size, img_size, 3), dtype=tf.float32)\r\n",
        "\r\n",
        "    x = inp\r\n",
        "    n_dwonsample = int(np.log2(img_size//256))\r\n",
        "    for _ in range(n_dwonsample):\r\n",
        "        x = blur2D(x)\r\n",
        "        x = tf.keras.layers.Conv2D(filter_num[x.shape[1]//2], 4, 2, padding='SAME', kernel_initializer=kernel_initializer)(x)\r\n",
        "        x = tf.nn.leaky_relu(x, 0.1)\r\n",
        "    mid_feature = []\r\n",
        "    for _ in range(5):\r\n",
        "        x = downscale(x, filter_num[x.shape[1]//2], attn_res=attn_res)\r\n",
        "        if x.shape[1] <=16:\r\n",
        "            mid_feature.append(x)\r\n",
        "    x = tf.keras.layers.Conv2D(filter_num[8], 1, kernel_initializer=kernel_initializer)(x)\r\n",
        "    x = tf.nn.leaky_relu(x, 0.1)\r\n",
        "    logits = tf.keras.layers.Conv2D(1, 4, kernel_initializer=kernel_initializer)(x)\r\n",
        "\r\n",
        "    # lr_part\r\n",
        "    # resize_inp = tf.image.resize(lin, (32, 32))\r\n",
        "    resize_inp = UpFIR2d(scale=img_size//32, k=[1, 3, 3, 1], down=True)(inp)\r\n",
        "    resize_inp.set_shape([None, 32, 32, 3])\r\n",
        "    x = tf.keras.layers.Conv2D(64, 3, padding='SAME', kernel_initializer=kernel_initializer)(resize_inp)\r\n",
        "    x += ScalingLayer()(global_self_attention(x, 64))\r\n",
        "    x = downscale(x, 32)\r\n",
        "    x += ScalingLayer()(global_self_attention(x, 32))\r\n",
        "    x = tfa.layers.AdaptiveAveragePooling2D((4, 4))(x)\r\n",
        "    lr_logits = tf.keras.layers.Conv2D(1, 4, kernel_initializer=kernel_initializer)(x)\r\n",
        "    return tf.keras.Model(inp, [logits, lr_logits]+mid_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg2w_52GdrKa"
      },
      "source": [
        "## Differentiable Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzyqm4H3si0E"
      },
      "source": [
        "# Differentiable Augmentation for Data-Efficient GAN Training\r\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\r\n",
        "# https://arxiv.org/pdf/2006.10738\r\n",
        "# https://github.com/mit-han-lab/data-efficient-gans\r\n",
        "\r\n",
        "def DiffAugment(x, policy=[], channels_first=False):\r\n",
        "    if policy:\r\n",
        "        if channels_first:\r\n",
        "            x = tf.transpose(x, [0, 2, 3, 1])\r\n",
        "        for p in policy:\r\n",
        "            for f in AUGMENT_FNS[p]:\r\n",
        "                x = f(x)\r\n",
        "        if channels_first:\r\n",
        "            x = tf.transpose(x, [0, 3, 1, 2])\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def rand_brightness(x):\r\n",
        "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) - 0.5\r\n",
        "    x = x + magnitude\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def rand_saturation(x):\r\n",
        "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) * 2\r\n",
        "    x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\r\n",
        "    x = (x - x_mean) * magnitude + x_mean\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def rand_contrast(x):\r\n",
        "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) + 0.5\r\n",
        "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\r\n",
        "    x = (x - x_mean) * magnitude + x_mean\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def rand_translation(x, ratio=0.125):\r\n",
        "    batch_size = tf.shape(x)[0]\r\n",
        "    image_size = tf.shape(x)[1:3]\r\n",
        "    shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\r\n",
        "    translation_x = tf.random.uniform([batch_size, 1], -shift[0], shift[0] + 1, dtype=tf.int32)\r\n",
        "    translation_y = tf.random.uniform([batch_size, 1], -shift[1], shift[1] + 1, dtype=tf.int32)\r\n",
        "    grid_x = tf.clip_by_value(tf.expand_dims(tf.range(image_size[0], dtype=tf.int32), 0) + translation_x + 1, 0, image_size[0] + 1)\r\n",
        "    grid_y = tf.clip_by_value(tf.expand_dims(tf.range(image_size[1], dtype=tf.int32), 0) + translation_y + 1, 0, image_size[1] + 1)\r\n",
        "    x = tf.gather_nd(tf.pad(x, [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_x, -1), batch_dims=1)\r\n",
        "    x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3]), [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_y, -1), batch_dims=1), [0, 2, 1, 3])\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def rand_cutout(x, ratio=0.5):\r\n",
        "    batch_size = tf.shape(x)[0]\r\n",
        "    image_size = x.shape[1:3]\r\n",
        "    cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\r\n",
        "    offset_x = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[0] + (1 - cutout_size[0] % 2), dtype=tf.int32)\r\n",
        "    offset_y = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[1] + (1 - cutout_size[1] % 2), dtype=tf.int32)\r\n",
        "    grid_batch, grid_x, grid_y = tf.meshgrid(tf.range(batch_size, dtype=tf.int32),\r\n",
        "                                             tf.range(cutout_size[0], dtype=tf.int32),\r\n",
        "                                             tf.range(cutout_size[1], dtype=tf.int32), indexing='ij')\r\n",
        "    cutout_grid = tf.stack([grid_batch, grid_x + offset_x - cutout_size[0] // 2, grid_y + offset_y - cutout_size[1] // 2], axis=-1)\r\n",
        "    mask_shape = tf.stack([batch_size, image_size[0], image_size[1]])\r\n",
        "    cutout_grid = tf.maximum(cutout_grid, 0)\r\n",
        "    cutout_grid = tf.minimum(cutout_grid, tf.reshape(mask_shape - 1, [1, 1, 1, 3]))\r\n",
        "    mask = tf.maximum(1 - tf.scatter_nd(cutout_grid, tf.ones([batch_size, cutout_size[0], cutout_size[1]], dtype=tf.float32), mask_shape), 0)\r\n",
        "    x = x * tf.expand_dims(mask, axis=3)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "AUGMENT_FNS = {\r\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\r\n",
        "    'translation': [rand_translation],\r\n",
        "    'cutout': [rand_cutout],\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMRMBWZY8oE-"
      },
      "source": [
        "## Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lldfJ5F28j6Q"
      },
      "source": [
        "class GAN:\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        save_dir,\n",
        "        batchsize=8,\n",
        "        grad_accumulation=4,\n",
        "        latent_dims=256,\n",
        "        lr=2e-4,\n",
        "        ttur_mul=1.0,\n",
        "        beta_1=0.5,\n",
        "        augment_policy=['translation', 'cutout'],\n",
        "        d_steps=2,\n",
        "        gp_type='r1',\n",
        "        gp_target=0.0,\n",
        "        gp_weight=10.0,\n",
        "        attn_res=[],\n",
        "        seed=12345):\n",
        " \n",
        "        self.save_dir = save_dir\n",
        "        self.batchsize = batchsize\n",
        "        self.grad_accumulation = grad_accumulation\n",
        "        self.batchsize_per_replica = self.batchsize\n",
        "        self.latent_dims = latent_dims\n",
        "        self.lr = lr\n",
        "        self.ttur_mul = ttur_mul\n",
        "        self.beta_1 = beta_1\n",
        "        self.d_steps = d_steps\n",
        "        self.gp_target = gp_target\n",
        "        self.gp_weight = gp_weight\n",
        "        self.policy = augment_policy\n",
        "        self.attn_res = attn_res\n",
        "        tf.random.set_seed(seed)\n",
        " \n",
        "        os.makedirs(os.path.join(self.save_dir, 'result', 'generated'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'result', 'reconstruction'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'result', 'part_reconstruction'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'models'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.save_dir, 'optimizers'), exist_ok=True)\n",
        "        with open(os.path.join(save_dir, 'hparams_info.txt'), 'w') as f:\n",
        "            f.write(f'batchsize : {batchsize}\\nlatent_dims : {latent_dims}\\nlearning_rater : {lr}\\nttur_mul : {ttur_mul}\\nAdam_beta_1 : {beta_1}\\ndiff_augment_policy : {augment_policy}\\ngp_type : {gp_type}\\ngp_target : {gp_target}\\ngp_weight : {gp_weight}\\nattn_res : {attn_res}\\nseed : {seed}\\n\\n')\n",
        " \n",
        "        print('Load dataset...')\n",
        "        self.dataset, self.data_shape, self.test_data = self.load_data(data_dir, self.batchsize*self.grad_accumulation)\n",
        "        self.test_z = tf.random.normal(shape=(25, self.latent_dims))\n",
        "        Image.fromarray(self.batch2tile(tf.image.resize(self.test_data, (128, 128)), 8, 8).numpy()).save(os.path.join(self.save_dir, 'result', 'reconstruction', 'raw.jpg'))\n",
        "        Image.fromarray(self.batch2tile(tf.image.resize(self.test_data, (256, 256)), 8, 8).numpy()).save(os.path.join(self.save_dir, 'result', 'part_reconstruction', 'raw.jpg'))\n",
        "        \n",
        "        self.model_names = ('G', 'D', 'whole_decoder', 'part_decoder')\n",
        "        self.G, self.D, self.dec, self.part_dec = self.initialize_models()\n",
        "        self.G_params = self.G.trainable_weights\n",
        "        self.D_params = self.D.trainable_weights + self.dec.trainable_weights + self.part_dec.trainable_weights\n",
        " \n",
        "        self.loss_names = ('real_score', 'fake_score', 'real_score_32', 'fake_score_32', 'whole_rec', 'part_rec', 'gp')\n",
        "        try:\n",
        "            with open(os.path.join(self.save_dir, 'result', 'losses.pkl'), 'rb') as file:\n",
        "                self.losses = pickle.load(file)\n",
        "                self.test_z = self.losses['test_z']\n",
        "        except:\n",
        "            self.losses = {key : [] for key in self.loss_names}\n",
        "            self.losses['iterations'] = 0\n",
        "            self.losses['d_iterations'] = 0\n",
        "            self.losses['test_z'] = self.test_z\n",
        "        if gp_type == 'r1':\n",
        "            self.gradient_penalty = self.r1_gradient_penalty\n",
        "        elif gp_type == 'r2':\n",
        "            self.gradient_penalty = self.r2_gradient_penalty\n",
        "        elif gp_type == 'wgan':\n",
        "            self.gradient_penalty = self.wgangp_gradient_penalty\n",
        "\n",
        "        self.G_opt, self.D_opt = self.init_optimizers()\n",
        "        self.it = tf.Variable(self.losses['iterations'], trainable=False, aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n",
        "        self.d_it = tf.Variable(self.losses['d_iterations'], trainable=False, aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n",
        "        self.G_opt.iterations.assign(self.losses['iterations'])\n",
        "        self.D_opt.iterations.assign(self.losses['iterations'])\n",
        "    \n",
        "    # def load_data(self, dir, batchsize=8):\n",
        "    #     with open(dir, 'rb') as file:\n",
        "    #         data = pickle.load(file)\n",
        "    #     dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    #     proc_fn = lambda x : tf.image.random_flip_left_right((tf.cast(x, tf.float32) / 127.5) - 1.0)\n",
        "    #     dataset = dataset.map(proc_fn).shuffle(4096, reshuffle_each_iteration=True).repeat().batch(batchsize, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    #     return dataset, data.shape, tf.cast(data[:64], tf.float32)/127.5 - 1.0\n",
        " \n",
        "    def load_data(self, dir, batchsize=8):\n",
        "        with open(dir, 'rb') as file:\n",
        "            data = pickle.load(file)\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "        def proc_fn(x):\n",
        "            img = tf.image.decode_jpeg(x)\n",
        "            img = tf.image.random_flip_left_right(img)\n",
        "            img = tf.cast(img, tf.float32)\n",
        "            img = img / 127.5 - 1.0\n",
        "            return img\n",
        "        dataset = dataset.map(proc_fn).shuffle(4096, reshuffle_each_iteration=True).repeat().batch(batchsize, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        test_data = tf.concat([tf.image.decode_jpeg(x)[np.newaxis, :, :, :] for x in data[:64]], axis=0)\n",
        "        test_data = tf.cast(test_data, tf.float32) / 127.5 - 1.0\n",
        "        return dataset, test_data.shape, test_data\n",
        " \n",
        "    def batch2tile(self, batch, row, col):\n",
        "        shape = batch.shape\n",
        "        batch = tf.transpose(tf.reshape(batch, [row, col]+shape[1:]), [0, 2, 1, 3, 4])\n",
        "        img = tf.reshape(batch, [shape[1]*row, shape[2]*col, -1])\n",
        "        return tf.cast((img + 1.0)*127.5, tf.uint8)\n",
        " \n",
        "    def initialize_models(self):\n",
        "        assert self.data_shape[1] in [256, 512, 1024]\n",
        "        if False not in [os.path.exists(os.path.join(self.save_dir, 'models', name+'.h5')) for name in self.model_names]:\n",
        "            print('Load models...')\n",
        "            custom_objects = {'ScalingLayer':ScalingLayer, 'GSA':GSA, 'UpFIR2d': UpFIR2d}\n",
        "            models = (tf.keras.models.load_model(os.path.join(self.save_dir, 'models', name+'.h5'), custom_objects=custom_objects, compile=False) for name in self.model_names)\n",
        "            return models\n",
        "        else:\n",
        "            print('Initialize models...')\n",
        "            G = create_generator(self.data_shape[1], attn_res=self.attn_res)\n",
        "            D = create_discriminator(self.data_shape[1], attn_res=self.attn_res)\n",
        "            d_output_shape = D.output_shape\n",
        "            dec_input = tf.keras.Input(shape=(8, 8, d_output_shape[-1][-1]), dtype=tf.float32)\n",
        "            dec = tf.keras.Model(dec_input, simple_decoder(dec_input))\n",
        "            partdec_input = tf.keras.Input(shape=(8, 8, d_output_shape[-2][-1]), dtype=tf.float32)\n",
        "            part_dec = tf.keras.Model(partdec_input, simple_decoder(partdec_input))\n",
        "            return G, D, dec, part_dec\n",
        "    \n",
        "    def init_optimizers(self):\n",
        "        print('Initialize optimizers...')\n",
        "        G_opt = tf.keras.optimizers.Adam(self.lr, beta_1=self.beta_1)\n",
        "        D_opt = tf.keras.optimizers.Adam(self.lr*self.ttur_mul, beta_1=self.beta_1)\n",
        "        for f_name, val, opt in zip(['G_opt', 'D_opt'], [self.G_params, self.D_params], [G_opt, D_opt]):\n",
        "            if os.path.exists(os.path.join(self.save_dir, 'optimizers', f_name+'.pkl')):\n",
        "                print('Load %s state...'%f_name)\n",
        "                with open(os.path.join(self.save_dir, 'optimizers', f_name+'.pkl'), 'rb') as file:\n",
        "                    opt_state = pickle.load(file)\n",
        "                with tf.name_scope(opt_state['opt_name']):\n",
        "                    w = opt_state['weights']\n",
        "                    for v in val:\n",
        "                        if v.name in w.keys():\n",
        "                            for slot in w[v.name].keys():\n",
        "                                initializer = tf.initializers.Constant(w[v.name][slot])\n",
        "                                opt.add_slot(v, slot, initializer=initializer)\n",
        "        return G_opt, D_opt\n",
        "    \n",
        "    # @tf.function\n",
        "    def sampling_image(self):\n",
        "        gen_img = self.random_generate(z=self.test_z)\n",
        "        _, _, f16, f8 = self.D(self.test_data, training=True)\n",
        "        rec = self.dec(f8, training=True)\n",
        "        quarter_feature = self.split_quarter(f16)\n",
        "        part = [self.part_dec(quarter_feature[i], training=True) for i in tf.range(4)]\n",
        "        top = tf.concat([part[0], part[1]], axis=2)\n",
        "        bottom = tf.concat([part[2], part[3]], axis=2)\n",
        "        part_rec = tf.concat([top, bottom], axis=1)\n",
        "        return gen_img, rec, part_rec\n",
        "    \n",
        "    def save_snapshot_image(self, epoch:int):\n",
        "        imgs = self.sampling_image()\n",
        "        file_names = [os.path.join(self.save_dir, 'result', 'generated', '%06d.jpg'%epoch),\n",
        "                      os.path.join(self.save_dir, 'result', 'reconstruction', '%06d.jpg'%epoch),\n",
        "                      os.path.join(self.save_dir, 'result', 'part_reconstruction', '%06d.jpg'%epoch)]\n",
        "        for img, path in zip(imgs, file_names):\n",
        "            row_col = int(np.sqrt(img.shape[0]))\n",
        "            Image.fromarray(self.batch2tile(img, row_col, row_col).numpy()).save(path)\n",
        " \n",
        "    def save(self):\n",
        "        for name, model in zip(self.model_names, [self.G, self.D, self.dec, self.part_dec]):\n",
        "            model.save(os.path.join(self.save_dir, 'models', name+'.h5'))\n",
        "        if self.losses['iterations'] % 10000 == 0:\n",
        "            os.makedirs(os.path.join(self.save_dir, 'models', str(self.losses['iterations'])), exist_ok=True)\n",
        "            for name, model in zip(self.model_names, [self.G, self.D, self.dec, self.part_dec]):\n",
        "                model.save(os.path.join(self.save_dir, 'models', str(self.losses['iterations']), name+'.h5'))\n",
        "        for opt, val, f_name in zip([self.G_opt, self.D_opt], [self.G_params, self.D_params], ['G_opt', 'D_opt']):\n",
        "            slot_names = opt.get_slot_names()\n",
        "            w = {}\n",
        "            for v in val:\n",
        "                w[v.name] = {}\n",
        "                for slot in slot_names:\n",
        "                    w[v.name][slot] = opt.get_slot(v, slot).numpy()\n",
        "            opt_weight = {'opt_name':opt._name, 'weights' : w}\n",
        "            with open(os.path.join(self.save_dir, 'optimizers', f_name+'.pkl'), 'wb') as file:\n",
        "                pickle.dump(opt_weight, file)\n",
        "                file.flush()\n",
        "        with open(os.path.join(self.save_dir, 'result', 'losses.pkl'), 'wb') as file:\n",
        "            pickle.dump(self.losses, file)\n",
        "            file.flush()\n",
        "    \n",
        "    def split_quarter(self, x):\n",
        "        shape = x.shape\n",
        "        x = tf.reshape(x, [-1, 2, shape[1]//2, 2, shape[2]//2, shape[3]])\n",
        "        x = tf.transpose(x, [1, 3, 0, 2, 4, 5])\n",
        "        x = tf.reshape(x, [4, -1, shape[1]//2, shape[2]//2, shape[3]])\n",
        "        return x\n",
        "    \n",
        "    def random_crop(self, feature, img):\n",
        "        idx = tf.random.uniform(shape=(), maxval=4, dtype=tf.int32)\n",
        "        img = tf.image.resize(img, (256, 256))\n",
        "        img = self.split_quarter(img)[idx]\n",
        "        feature = self.split_quarter(feature)[idx]\n",
        "        return feature, img\n",
        "    \n",
        "    def get_latent_z(self, z=None, n=None, normalize=True):\n",
        "        if z is None:\n",
        "            z = tf.random.normal(shape=(n, self.latent_dims), dtype=tf.float32)\n",
        "        if normalize:\n",
        "            z = z/tf.norm(z, axis=-1, keepdims=True)\n",
        "        return z\n",
        " \n",
        "    def random_generate(self, z=None, n=None, normalize_latent=True, training=True):\n",
        "        z = self.get_latent_z(z=z, n=n, normalize=normalize_latent)\n",
        "        img = self.G(z, training=training)\n",
        "        return img\n",
        "    \n",
        "    def mixing_generate(self, z_list=None, n=None, normalize_latent=True, training=True):\n",
        "        lr = self.G.get_layer(index=1)\n",
        "        hr = self.G.get_layer(index=2)\n",
        "        if z_list is None:\n",
        "            z_list = [self.get_latent_z(n=n, normalize=normalize_latent) for _ in range(2)]\n",
        "        else:\n",
        "            z_list = [self.get_latent_z(z, normalize=normalize_latent) for z in z_list]\n",
        "        hr_inputs_z0 = lr(z_list[0], training=training)\n",
        "        hr_inputs_z1 = lr(z_list[1], training=training)\n",
        "        idx = tf.random.uniform(shape=(), maxval=len(lr.output_shape), dtype=tf.int32)\n",
        "        hr_inputs = hr_inputs_z0[:idx] + hr_inputs_z1[idx:]\n",
        "        img = hr(hr_inputs, training=training)\n",
        "        return img\n",
        " \n",
        "    def r1_gradient_penalty(self, real, fake):\n",
        "        bs = real.shape\n",
        "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "            tape.watch(real)\n",
        "            aug_image = DiffAugment(real, self.policy)\n",
        "            d_out, d_out32, _, _  = self.D(aug_image, training=True)\n",
        "        grad = tape.gradient([d_out, d_out32], real)\n",
        "        gp = tf.norm(tf.reshape(grad, [-1, bs[1]*bs[2]*bs[3]]), axis=-1)\n",
        "        gp = (gp - self.gp_target)**2\n",
        "        return gp\n",
        "    \n",
        "    def r2_gradient_penalty(self, real, fake):\n",
        "        bs = real.shape\n",
        "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "            tape.watch(fake)\n",
        "            aug_image = DiffAugment(fake, self.policy)\n",
        "            d_out, d_out32, _, _  = self.D(aug_image, training=True)\n",
        "        grad = tape.gradient([d_out, d_out32], fake)\n",
        "        gp = tf.norm(tf.reshape(grad, [-1, bs[1]*bs[2]*bs[3]]), axis=-1)\n",
        "        gp = (gp - self.gp_target)**2\n",
        "        return gp\n",
        "    \n",
        "    def wgangp_gradient_penalty(self, real, fake):\n",
        "        bs = real.shape\n",
        "        alpha = tf.random.uniform(shape=[bs[0], 1, 1, 1])\n",
        "        image = real * alpha + fake * (1 - alpha)\n",
        "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "            tape.watch(image)\n",
        "            aug_image = DiffAugment(image, self.policy)\n",
        "            d_out, d_out32, _, _  = self.D(aug_image, training=True)\n",
        "        grad = tape.gradient([d_out, d_out32], image)\n",
        "        gp = tf.norm(tf.reshape(grad, [-1, bs[1]*bs[2]*bs[3]]), axis=-1)\n",
        "        gp = (gp - self.gp_target)**2\n",
        "        return gp\n",
        "    \n",
        "    def hinge_loss(self, real_logits, fake_logits):\n",
        "        real_loss = tf.reduce_mean(tf.nn.relu(1.0 + real_logits), axis=[1, 2, 3])\n",
        "        fake_loss = tf.reduce_mean(tf.nn.relu(1.0 - fake_logits), axis=[1, 2, 3])\n",
        "        return real_loss + fake_loss\n",
        "    \n",
        "    def reconstruction_loss(self, rec, real):\n",
        "        return tf.reduce_mean((real - rec)**2, axis=[1, 2, 3])\n",
        "    \n",
        "    def _train_step(self, real_img):\n",
        "        bs = real_img.shape[0]\n",
        "        with tf.GradientTape(watch_accessed_variables=False) as g_tape:\n",
        "            g_tape.watch(self.G_params)\n",
        "            # fake_img = self.mixing_generate(n=bs)\n",
        "            fake_img = self.random_generate(n=bs)\n",
        "            fake_aug = DiffAugment(fake_img, self.policy)\n",
        "        for _ in range(self.d_steps):\n",
        "            with tf.GradientTape() as d_tape:\n",
        "                real_aug = DiffAugment(real_img, self.policy)\n",
        "                fake_score, fake_score32, _, _ = self.D(fake_aug, training=True)\n",
        "                real_score, real_score32, f16, f8 = self.D(real_aug, training=True)\n",
        " \n",
        "                d_adv_loss = self.hinge_loss(real_score, fake_score)\n",
        "                d_adv_loss_32 = self.hinge_loss(real_score32, fake_score32)\n",
        " \n",
        "                rec_loss = self.reconstruction_loss(self.dec(f8, training=True), tf.image.resize(real_aug, (128, 128)))\n",
        "                crop_feature, crop_img = self.random_crop(f16, real_aug)\n",
        "                part_rec_loss = self.reconstruction_loss(self.part_dec(crop_feature, training=True), crop_img)\n",
        " \n",
        "                d_loss = d_adv_loss + d_adv_loss_32 + rec_loss + part_rec_loss\n",
        " \n",
        "                d_loss = tf.reduce_sum(d_loss) * (1. / self.batchsize)\n",
        "            d_grad = d_tape.gradient(d_loss, self.D_params, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        " \n",
        "            # gp\n",
        "            if self.d_it % 4 == 0:\n",
        "                with tf.GradientTape() as d_tape:\n",
        "                    gp = self.gradient_penalty(real_img, fake_img)\n",
        "                    gp_loss = self.gp_weight * tf.reduce_sum(gp) * (1. / self.batchsize)\n",
        "                gp_grad = d_tape.gradient(gp_loss, self.D_params, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        "                d_grad = [x+y for x, y in zip(d_grad, gp_grad)]\n",
        "            else:\n",
        "                gp = self.gradient_penalty(real_img, fake_img)\n",
        "            self.D_opt.apply_gradients(zip(d_grad, self.D_params))\n",
        "            self.d_it.assign_add(1)\n",
        "        with g_tape:\n",
        "            fake_score, fake_score32, _, _ = self.D(fake_aug, training=True)\n",
        "            fake_score, fake_score32 = tf.reduce_mean(fake_score, axis=[1, 2, 3]), tf.reduce_mean(fake_score32, axis=[1, 2, 3])\n",
        " \n",
        "            g_loss = fake_score + fake_score32\n",
        "            g_loss = tf.reduce_sum(g_loss) * (1. / self.batchsize)\n",
        "        g_grad = g_tape.gradient(g_loss, self.G_params, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        "        self.G_opt.apply_gradients(zip(g_grad, self.G_params))\n",
        "        return tuple(tf.reduce_mean(x) for x in (real_score, fake_score, real_score32, fake_score32, rec_loss, part_rec_loss, gp))\n",
        "\n",
        "    def _train_step_gradacc(self, real_img):\n",
        "        total_real_score = tf.constant(0, dtype=tf.float32)\n",
        "        total_fake_score = tf.constant(0, dtype=tf.float32)\n",
        "        total_real_score32 = tf.constant(0, dtype=tf.float32)\n",
        "        total_fake_score32 = tf.constant(0, dtype=tf.float32)\n",
        "        total_rec_loss = tf.constant(0, dtype=tf.float32)\n",
        "        total_part_rec_loss = tf.constant(0, dtype=tf.float32)\n",
        "        total_gp = tf.constant(0, dtype=tf.float32)\n",
        "\n",
        "        d_grad = [tf.zeros_like(v) for v in self.D_params]\n",
        "        for batch in tf.split(real_img, self.grad_accumulation, axis=0):\n",
        "            bs = batch.shape[0]\n",
        "            fake_img = self.random_generate(n=bs)\n",
        "            with tf.GradientTape() as d_tape:\n",
        "                real_aug = DiffAugment(batch, self.policy)\n",
        "                fake_score, fake_score32, _, _ = self.D(DiffAugment(fake_img, self.policy), training=True)\n",
        "                real_score, real_score32, f16, f8 = self.D(real_aug, training=True)\n",
        " \n",
        "                d_adv_loss = self.hinge_loss(real_score, fake_score)\n",
        "                d_adv_loss_32 = self.hinge_loss(real_score32, fake_score32)\n",
        " \n",
        "                rec_loss = self.reconstruction_loss(self.dec(f8, training=True), tf.image.resize(real_aug, (128, 128)))\n",
        "                crop_feature, crop_img = self.random_crop(f16, real_aug)\n",
        "                part_rec_loss = self.reconstruction_loss(self.part_dec(crop_feature, training=True), crop_img)\n",
        " \n",
        "                d_loss = d_adv_loss + d_adv_loss_32 + rec_loss + part_rec_loss\n",
        " \n",
        "                d_loss = tf.reduce_sum(d_loss) * (1. / (self.batchsize * self.grad_accumulation))\n",
        "            \n",
        "            d_grad = [x + y for x, y in zip(d_grad, d_tape.gradient(d_loss, self.D_params, unconnected_gradients=tf.UnconnectedGradients.ZERO))]\n",
        "\n",
        "            total_real_score += tf.reduce_mean(real_score)\n",
        "            total_real_score32 += tf.reduce_mean(real_score32)\n",
        "            total_rec_loss += tf.reduce_mean(rec_loss)\n",
        "            total_part_rec_loss += tf.reduce_mean(part_rec_loss)\n",
        "            \n",
        "            # gp\n",
        "            if self.it % 4 == 0:\n",
        "                with tf.GradientTape() as d_tape:\n",
        "                    gp = self.gradient_penalty(batch, fake_img)\n",
        "                    gp_loss = self.gp_weight * tf.reduce_sum(gp) * (1. / (self.batchsize * self.grad_accumulation))\n",
        "                d_grad = [x + y for x, y in zip(d_grad, d_tape.gradient(gp_loss, self.D_params, unconnected_gradients=tf.UnconnectedGradients.ZERO))]\n",
        "            else:\n",
        "                gp = self.gradient_penalty(batch, fake_img)\n",
        "            total_gp += tf.reduce_mean(gp)\n",
        "        self.D_opt.apply_gradients(zip(d_grad, self.D_params))\n",
        "\n",
        "        g_grad = [tf.zeros_like(v) for v in self.G_params]\n",
        "        for _ in tf.range(self.grad_accumulation):\n",
        "            with tf.GradientTape(watch_accessed_variables=False) as g_tape:\n",
        "                g_tape.watch(self.G_params)\n",
        "                fake_img = self.random_generate(n=self.batchsize_per_replica)\n",
        "                fake_score, fake_score32, _, _ = self.D(DiffAugment(fake_img, self.policy), training=True)\n",
        "                fake_score, fake_score32 = tf.reduce_mean(fake_score, axis=[1, 2, 3]), tf.reduce_mean(fake_score32, axis=[1, 2, 3])\n",
        "    \n",
        "                g_loss = fake_score + fake_score32\n",
        "                g_loss = tf.reduce_sum(g_loss) * (1. / (self.batchsize * self.grad_accumulation))\n",
        "            g_grad = [x+y for x, y in zip(g_grad, g_tape.gradient(g_loss, self.G_params, unconnected_gradients=tf.UnconnectedGradients.ZERO))]\n",
        "            total_fake_score += tf.reduce_mean(fake_score)\n",
        "            total_fake_score32 += tf.reduce_mean(fake_score32)\n",
        "        self.G_opt.apply_gradients(zip(g_grad, self.G_params))\n",
        "        return tuple(x / self.grad_accumulation for x in (total_real_score, total_fake_score, total_real_score32, total_fake_score32, total_rec_loss, total_part_rec_loss, total_gp))\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(self, img, strategy=None):\n",
        "        if strategy is not None:\n",
        "            result = strategy.run(self._train_step, args=(img,))\n",
        "            result = (strategy.reduce(tf.distribute.ReduceOp.MEAN, r, axis=None) for r in result)\n",
        "        else:\n",
        "            result = self._train_step(img)\n",
        "        return tuple(result)\n",
        "    \n",
        "    def train_TPU(self, strategy=None, iterations=50000):\n",
        "        if strategy is not None:\n",
        "            self.dataset = strategy.experimental_distribute_dataset(self.dataset)\n",
        "            self.batchsize_per_replica = self.batchsize // strategy.num_replicas_in_sync\n",
        "        else:\n",
        "            self.batchsize_per_replica = self.batchsize\n",
        "        iterater = iter(self.dataset)\n",
        " \n",
        "        with tqdm(range(self.losses['iterations']+1, iterations+1)) as p_bar:\n",
        "            for _ in p_bar:\n",
        "                data = next(iterater)\n",
        "                loss = self.train_step(data, strategy=strategy)\n",
        "                loss_dict = {'iteration':self.losses['iterations']+1}\n",
        "                for name, val in zip(self.loss_names, loss):\n",
        "                    l = val.numpy()\n",
        "                    self.losses[name].append(l)\n",
        "                    loss_dict[name] = l\n",
        "                p_bar.set_postfix(loss_dict)\n",
        "                if self.losses['iterations'] % 1000 == 0:\n",
        "                    self.save_snapshot_image(self.losses['iterations'])\n",
        "                    self.save()\n",
        "                self.it.assign_add(1)\n",
        "                self.losses['iterations'] = self.it.numpy()\n",
        "                self.losses['d_iterations'] = self.d_it.numpy()\n",
        "            self.save_snapshot_image(self.losses['iterations'])\n",
        "            self.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu36ECuwm6KB"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk4bWWxwiq4i"
      },
      "source": [
        "# Download Dataset\r\n",
        "%%shell\r\n",
        "\r\n",
        "FILE_ID=1-0DvJgxitJtoVYL9HwYgKD3_5GpTDSYK\r\n",
        "FILE_NAME=dataset.pkl\r\n",
        "curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=${FILE_ID}\" > /dev/null\r\n",
        "CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"  \r\n",
        "curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=${FILE_ID}\" -o ${FILE_NAME}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvA_IyLceH7Q"
      },
      "source": [
        "args = dict(\r\n",
        "    data_dir = 'dataset.pkl',\r\n",
        "    save_dir = 'LightweightGAN_animeface',\r\n",
        "    batchsize = 16,\r\n",
        "    latent_dims = 256,\r\n",
        "    lr = 2e-4,\r\n",
        "    ttur_mul = 1.0,\r\n",
        "    # learning rate in bigGAN\r\n",
        "    #   res 128x128\r\n",
        "    #   D : 2e-4 G : 5e-5\r\n",
        "    #   res 256x256 or higher\r\n",
        "    #   both D and G : 2.5e-5\r\n",
        "    # learning rate in SAGAN\r\n",
        "    #   D : 4e-4 G : 1e-4\r\n",
        "    gp_type='r1',       # select 'r1' or 'r2' or 'wgan'\r\n",
        "    gp_target = 1.0,    # gradient penalty center \r\n",
        "    gp_weight = 10.0,\r\n",
        "    d_steps = 1,\r\n",
        "    augment_policy=['translation', 'cutout'],\r\n",
        "    attn_res=[],\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXtKICLbwoH"
      },
      "source": [
        "## Train on TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DueQaFRcbzNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef9123b-3f4d-423d-b8f1-89a603ea4970"
      },
      "source": [
        "# setup TPU\r\n",
        "tpu_grpc_url = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n",
        "tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\r\n",
        "tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\r\n",
        "strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.43.221.178:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.43.221.178:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KUAbPv9cqz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6aa5f5-b914-4cce-b521-71a4b60a2839"
      },
      "source": [
        "tf.keras.backend.clear_session()\r\n",
        "with strategy.scope():\r\n",
        "    gan = GAN(**args)\r\n",
        "    gan.train_TPU(strategy, iterations=300000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load dataset...\n",
            "Load models...\n",
            "Initialize optimizers...\n",
            "Load G_opt state...\n",
            "Load D_opt state...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|██████▊   | 85112/124000 [10:25:12<4:29:17,  2.41it/s, iteration=261113, real_score=-.374, fake_score=0.249, real_score_32=0.345, fake_score_32=0.557, whole_rec=0.0417, part_rec=0.0334, gp=0.00538]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}